readme_content = """
# ğŸ“ TELECOM CUSTOMER CHURN PREDICTION

## ğŸ† OVERVIEW

This project aims to predict **customer churn** for a leading telecom operator using machine learning. The objective is to empower customer retention teams with insights and risk scores that help prioritize outreach and reduce revenue loss.

---

## ğŸ“‚ DATASET

### ğŸ“Š Attributes:
- `State` â€“ Customer location
- `Account Length` â€“ Duration of the customerâ€™s subscription
- `Area Code` â€“ Regional identifier
- `International Plan` â€“ Whether international calls are enabled
- `VMail Plan` â€“ Voice mail subscription
- `Day/Eve/Night/Intl Mins/Calls/Charge` â€“ Usage metrics
- `CustServ Calls` â€“ Number of customer service interactions
- `Churn` â€“ ğŸ”º Target variable (Yes = customer churned)

---

## ğŸ“Œ BUSINESS OBJECTIVES

1. Predict whether a customer will **churn** or **stay**
2. Identify key drivers of churn
3. Generate **churn risk scores** to guide customer care prioritization

---

## âš™ï¸ PREPROCESSING TECHNIQUES

- ğŸ”¹ Handled missing values
- ğŸ”¹ Label encoding of categorical variables
- ğŸ”¹ Outlier treatment using IQR and median imputation
- ğŸ”¹ Normalization using `MinMaxScaler` and `StandardScaler`
- ğŸ”¹ Feature selection based on importance
- ğŸ”¹ Train-test split using stratified sampling

---

## âš ï¸ CLASS IMBALANCE

Since only ~14% of customers churn, the dataset is imbalanced. This was addressed via:

- ğŸ”¸ **Under-sampling**
- ğŸ”¸ **Over-sampling (SMOTE)**
- ğŸ”¸ **Class weights**

---

## âš¡ MODEL PERFORMANCE COMPARISON

| Model Type                 | Model Name       | Accuracy | F1 Score |
|---------------------------|------------------|----------|----------|
| âœ… **Best Overall**        | Random Forest     | **96.71%** | **96.51%** |
| ğŸŸ¡ Under-sampling Model    | Bagging Tree      | 92.81%   | 93.15%   |
| ğŸ”µ SMOTE (Over-sampling)   | XGBoost           | 95.76%   | 95.59%   |
| ğŸ”¹ Baseline Model          | Logistic Regression | 88%    | 68%      |

---

## âœ… FINAL RECOMMENDATION:

1. **Best Overall Model:**  
   **Random Forest**  
   *Accuracy: 96.71% | F1 Score: 96.51%*  
   **Reason:** Achieves the highest accuracy and balanced performance across churn and non-churn classes.

2. **Best Model with Under-Sampling:**  
   **Bagging Tree**  
   *Accuracy: 92.81% | F1 Score: 93.15%*  
   **Reason:** Performs well even after handling imbalance through under-sampling.

3. **Best Model with SMOTE Over-Sampling:**  
   **XGBoost**  
   *Accuracy: 95.76% | F1 Score: 95.59%*  
   **Reason:** Handles class imbalance effectively and achieves very high prediction scores.

---

## ğŸ“Š CHURN RISK SCORE INSIGHTS

1. High churn risk customers (score ~1.0) require **urgent retention** measures.
2. Moderate risk customers (score 0.53â€“0.63) are fewer and can be managed with **targeted but less intensive** strategies.
3. Resource allocation should focus on **high-risk** first to maximize retention ROI.

---

## ğŸ“Š EVALUATION METRICS

- **Accuracy** â€“ Overall correct predictions
- **F1-Score** â€“ Balanced score between precision & recall
- **ROC-AUC** â€“ Ability to distinguish between churned & non-churned customers

---

## ğŸ“Œ KEY INSIGHTS

- Customers with **International Plans** and frequent **Customer Service Calls** are more likely to churn.
- Random Forest and XGBoost deliver the most robust and generalizable predictions.
- Churn risk scoring can guide **retention resource allocation** effectively.

---

## ğŸ›  INSTALLATION

```bash
pip install numpy pandas scikit-learn xgboost seaborn matplotlib imbalanced-learn
